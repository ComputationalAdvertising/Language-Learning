title: "第03章：马尔科夫决策过程" 
mathjax: true
date: 2017-09-13 22:43:49
categories: 
	- 强化学习与智能决策
tags: 
	- Agent
	- MDP
	- Markov Decision Process
---

+ author: zhouyongsdzh@foxmail.com
+ date: 2016-09-26
+ weibo: [@周永_52ML](http://weibo.com/p/1005051707438033/home?)

## 目录

1. [智能体与环境](#1.智能体与环境)
2. [马尔科夫决策过程](#2.马尔科夫决策过程)

<h2 id="1.智能体与环境">1. 智能体与环境</h2>

强化学习问题不同于传统机器学习问题，它是一种在**交互**的过程中学习并实现目标的问题。这里把具有学习能力和决策能力的程序或系统称之为Agent（代理，智能体）；与之交互的对象统称为环境（Environment）。交互过程如下图所示：

<p style="text-align:center"><img src="https://github.com/ComputationalAds/caml-material/blob/master/picture/reinforcement-learning/chapter1-reinforcement-learning/chapter1-pic02-agent-envs-interaction.png?raw=true" width = "550" height = "200" alt="chapter1-pic02-agent-envs-interaction"/>


智能体与环境的交互通常是在连续的“时间”（discrete time steps）过程中进行的，$t=0,1,2, \cdots$。 在每个时间步骤$t$，智能体会针对当前环境状态$S_t$，并基于此执行一个动作$A_t$，在下一时刻$t+1$, 环境在动作$A_t$的作用下会产生新的状态$S_{t+1}$。同时智能体也会接收到一个回报$R_{t+1}$。持续地交互过程中，Agent与Environment会产生大量的数据。智能体中的算法利用产生的数据调整自身的动作策略，再与环境交互，产生新的数据，并利用新的数据进一步优化自身策略。经过循环往复，智能体最终学习到相应任务整体收益最大化对应的最优策略。

在每一步中，智能体都要完成从状态到选择每个可能动作的映射。这个映射过程称为智能体的**决策（policy，用符号$\pi_t$表示）**。那么$\pi_t(A_t = a|S_t = s)$表示状态为$S_t=s$时选择制定动态$A_t = a$的概率。

强化学习解决的是在交互过程中以目标为导向的学习问题，目标就是长期收益最大化。无论学习的场景是什么、目标是什么，以目标为导向的行为学习问题都可以归纳为3个主要的“信号”（signal）：

+ Actions: 动作，智能体的选择集合；
+ States: 状态，智能体从环境接收到的信息；
+ Rewards: 回报，智能体的优化目标。

无数学者们经过几十年不断的努力和探索，提出了一套可以解决大部分强化学习问题的框架，这个框架就是**马尔可夫决策过程（Markov Decision Process, 简称MDP）**. 下面会循序渐进地介绍MDP过程。

> **强化学习与传统机器学习**
传统机器学习假设算法本身与环境无影响，强化学习破除了这一限制，考虑到了算法对于环境的影响，这使得强化学习特别适合解决**多回合博弈or顺序决策问题**（sequential decision making）。
在强化学习中，数据是在运行过程中自主收集。这解决了机器学习中常见的训练数据难于获取的问题。**强化学习的算法具有自主学习能力**，这就是强化学习在机器人领域里面使用比较广泛的原因。但是要注意到的是，通常强化学习的速度限制并不是来自于算法的训练，而是来自于环境给予反馈的延时。

<h2 id="2.马尔科夫决策过程">2. 马尔科夫决策过程</h2>


介绍MDP过程之前，我们先了解一些基本概念：状态、随机过程、马尔可夫性、马尔可夫过程。


马尔可夫性：所谓马尔可夫性是指系统的下一个状态\\(s_{t+1}\\)仅与当前状态$s_t$有关，而与以前的状态无关。

定义：**状态$s_t$是马尔可夫的，当且仅当 $p(s_{t+1}|s_t) = p(s_{t+1}|s_t, s_{t-1}, \cdots,s_t)$**。

从定义中可以看到，当前状态$s_t$其实蕴含了所有相关的历史信息$s_1, \cdots, s_t$。 马尔可夫性描述的是每个状态的性质，但真正有用的是如何描述一个**状态序列**。数学中用来描述随机变量序列的学科叫随机过程（本科数学系or研究生数学课程）。所谓随机过程就是指**随机变量序列**。

如果随机变量序列中的每个状态都满足马尔可夫性，那么此随机过程称为**马尔可夫随机过程**。


**马尔可夫过程**


定义：马尔可夫过程用一个二元组$(\mathcal{S}, \mathcal{P})$表示，且满足：$\mathcal{S}$是有限状态集合，$\mathcal{P}$是状态转移概率集合。对应的状态转移概率矩阵为：

$$
P = \begin{bmatrix} P_{11} & \cdots & P_{1n} \\\ \vdots & \ddots & \vdots \\\ P_{n1} & \cdots & P_{nn} \\\ \end{bmatrix} 
$$

下面用一个例子来直观地理解。


//TODO 待补充


以上状态序列称为马尔可夫链。当给定状态转移概率时，从某个状态出发存在多条马尔可夫链。

**马尔可夫决策过程**

对于智能体（可以是机器人）而言，马尔可夫过程不足以描述其特点，因为他们都是通过动作与环境进行交互，并从环境中获得奖励，而**马尔可夫过程不存在动作和奖励**。那么，**将动作（策略）和奖励（回报）考虑在内的马尔可夫过程称为马尔可夫决策过程。**

马尔可夫决策过程可以用四元组$(\mathcal{S}, \mathcal{A}, \mathcal{P}, R)$描述，其中：$\mathcal{S}$为有限的状态集合，$\mathcal{A}$为有限的动作集合，$\mathcal{P}$为状态转移概率集合，$R$为回报函数。与马尔可夫过程不同的是，**马尔可夫决策过程的状态转移概率是包含动作的**，即：$P_{ss'}^{a} = P(S_{t+1}=s' | S_t = s, A_t = a)$.


强化学习的目标是**给定一个马尔可夫决策过程，寻找最优策略**。策略即是状态到动作的映射，这里用符号$\pi$表示，它是指给定状态$s$时，动作在动作集合中的分布。即：$\pi(a|s) = p(A_t = a | S_t = s)$。

可以看到，策略的定义是用条件概率分布给出的。策略$\pi$在每个状态$s$指定一个动作概率。可以想象，如果给出的策略$\pi$是确定性的，那么策略$\pi$在每个状态$s$指定一个确定的动作。

符合MDP过程强化学习问题的优化目标是什么？寻找最优策略。并且在给定策略下，可以评估`状态`或`状态－动作对`的值函数。

**累积回报**

前面提到，智能体的目标是最大化长期受到的累积回报（cumulative reward）。如何定义累计回报呢？如果在时间$t$时刻之后，智能体接受到的回报序列表示成$R_{t+1}, R_{t+2}, \cdots, R_T$. 那么，我们寻求的是最大化期望回报（expected return），这里用$G_t$表示特定的回报序列函数。回报的表示方法之一是将回报累加，即：

$$
G_t = R_{t+1} + R_{t+2} + \cdots + R_T
$$

考虑到实际场景，在计算累积回报时都会引入一个折扣因子（用$\gamma$表示)。智能体尝试选择一个动作使得未来的折扣回报累积最大化。特别的，智能体会在时刻$t$选择动作$A_t$能最大化期望折扣回报（expected discounted return）。即：

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t} R_{t+k+1} = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}
$$

上式包括$T=\infty$或者$\gamma=1$的可能（但是两者不能同时满足）。 由于策略$\pi$是随机的，所以累积回报也是随机的。**为了评价状态$s$的价值，我们需要定义一个确定量来描述状态$s$的价值，很自然的想法是利用累积回报来衡量状态$s$的价值**。然而，累积回报$G_t$是个随机变量，不是一个确定值，因此无法进行描述。但是**累积回报的期望是个确定值**，可以作为状态值函数的定义。

**值函数**

几乎所有的强化学习算法都涉及到评估值函数(`状态值函数`或者`状态-动作值函数`)，因为在**强化学习中知识是存储在值函数**中的。值函数用来评估智能体在给定`状态`或`状态－动作对`下表现的会有“多好”。这里的“多好”用累积回报的期望来衡量。因此，我们可以说：**值函数=累积回报的期望，最优值函数是我们的寻找目标**。

> 所谓深度强化学习是利用深度神经网络来逼近值函数的一类强化学习方法。AlphaGo Zero使用了华人科学家何凯明等人提出的**深度残差网络（ResNet）**进行深度学习。

这里用$\pi$表示策略，即状态到动作的映射。用$\pi(a|s)$表示当状态为$s$时，采取动作$a$的概率(条件概率分布)。在策略$\pi$下状态$s$的值函数(状态值函数)用$v_{\pi}(s)$表示。对于MDP过程，$v_{\pi}(s)$形式化表示为：

$$
v_{\pi}(s) = E_{\pi}[G_t \,|\, S_t = s] = E_{\pi} \left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \big| S_t = s \right]
$$

这里$E_{\pi}[\cdot]$表示在策略$\pi$下状态$s$的期望值（最终状态的函数值总是为0）。我们**称$v_{\pi}$为策略$\pi$对应的状态值函数（state-value function）**。

> 定义：所有策略中对应的最大状态值函数称为**最优状态值函数**，表示为$v_{*}(s) = \max_{\pi} v_{\pi}(s)$.


类似的，在策略$\pi$下，状态为$s$时动作$a$对应值函数 我们称之为**状态动作值函数（action-value function）**，用$q_{\pi}(s,a)$表示为：

$$
q_{\pi}(s, a) = E_{\pi}[G_t \,|\, S_t = s, A_t = a] = E_{\pi} \left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \big| S_t = s, A_t = a\right]
$$

> 定义：所有策略中对应的最大状态动作值函数称为**最优状态动作值函数**，表示为$q_{*}(s,a) = \max_{\pi} q_{\pi}(s,a)$.

计算值函数的目的就是为了构建学习算法从数据中学到最优策略。**策略与值函数一一对应**，最优策略对应最优值函数。

**贝尔曼方程**

贝尔曼方程建立了当前状态值函数$v_{\pi}(S_{t})$与下一状态值函数$v_{\pi}(S_{t+1})$之间的关系，使得值函数可以被分解成子问题求解。以状态值函数为例：

$$
\begin{align}
v_{\pi}(s) & = E_{\pi} \left[G_t | S_t = s \right] \\\
& = E_{\pi} \left[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \cdots) | S_t = s \right] \\\
& = E_{\pi} \left[R_{t+1} + \gamma G_{t+1} | S_t = s\right] \\\
& = E_{\pi} \left[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s\right]
\end{align}
$$

可以看到，值函数可以被拆成两部分：**立即奖赏$R_{t+1}$和下一状态折扣后的值函数$\gamma v_{\pi}(S_{t+1})$**。

类似的，状态动作值函数也满足贝尔曼方程，即：

$$
q_{\pi}(s,a) = E_{\pi} \left[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a \right]
$$

状态值函数与状态动作值函数的关系：

$$
\begin{align}
v_{\pi}(s) & = \sum_{a \in \mathcal{A}} \pi(a|s) \cdot q_{\pi}(s,a) \\\
q_{\pi}(s,a) & = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) v_{\pi}(s')
\end{align}
$$

因此，我们进一步可以得到：

$$
\begin{align}
v_{\pi}(s) & = \sum_{a \in \mathcal{A}} \pi(a|s) \left(R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) v_{\pi}(s') \right) \\\
q_{\pi}(s,a) &= R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) \sum_{a' \in \mathcal{A}} \pi(a'|s') \cdot q_{\pi}(s', a') 
\end{align}
$$

这里$\pi(a|s)$表示状态$s$对应不同的策略（动作集合的子集），$R(s,a)$表示状态$s$下执行动作$a$得到的立即奖赏；$P(s'|s,a)$表示执行动作$a$后下一步的状态概率。

分别记最优策略$\pi_*$对应的状态值函数和状态动作值函数为$v_*(s)$和$q_*(s,a)$，它们之间的关系满足：$v_*(s) = \max_{a} q_*(s,a)$。因此，状态值函数对应的贝尔曼最优方程为： 

$$
v_*(s) = \max_{a} \left(R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) v_*(s') \right)
$$

状态动作值函数对应的贝尔曼最优方程为：

$$
q_*(s,a) = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) \max_{a' \in \mathcal{A}} q_*(s', a')
$$

**最优策略（Optimal Policy）**

先看最优策略的定义：**如果策略$\pi$是最优的，但且仅当对于任意的状态$s$, 均存在策略$\pi$对应的值函数大于等于其它策略的值函数**。用公式表示为：$\pi \ge \pi', \text{if}\; v_{\pi}(s) \ge v_{\pi'}(s), \forall s$.

对于任意的马尔科夫决策过程，以下条件均成立：

1. 最优策略$\pi_*$均不差于所有其他的策略，形式化表示：$\pi_* \ge \pi, \forall \pi$;
2. 所有的最优策略均对应着最优状态值函数，形式化表示：$v_{\pi_*}(s) = v_*(s)$;
3. 所有的最优策略均对应着最优状态动作值函数，形式化表示：$q_{\pi_*}(s,a) = q_*(s,a)$;

以上3条就是**策略最优定理**。最优策略满足公式（状态值函数对应的贝尔曼最优方程），**最优策略可以通过最大化动作状态值函数贝尔曼最优方程**。即：

$$
\pi_*(a|s) = \left \{ \begin{array}{ll} 1, \quad \text{if a = arg} \max_{a \in \mathcal{A}} q_*(s,a) \\\
0, \quad otherwise. \\\
\end{array} \right.
$$

**如何通过贝尔曼最优方程求解最优策略？**

参考《第04章：动态规划》





