title: "第08章：深入浅出ML之Reinforcement Learning家族"
mathjax: true
date: 2016-02-28 21:31:36
categories: 
	- 深入浅出机器学习
tags: 
	- 任务与奖赏
	- 马尔可夫决策过程
	- epsilon-贪心法
	- 摇臂赌博机
---

+ author: zhouyongsdzh@foxmail.com
+ date: 2016-02-18
+ sina weibo: [@周永_52ML](http://weibo.com/p/1005051707438033/home?)

---

**内容列表**

+ 写在前面
+ 马尔可夫决策过程
+ \\(\epsilon\\)-贪心法
+ ...

<br>
### 写在前面
---

强化学习这个领域是我在接触机器学习一年多才开始真正关注的，虽然读书的时候修过《高级人工智能》这门课，里面提到了Q-Learning，但当时没怎么重视，也没学好。

直到快毕业那年，接触了一个互联网广告中的一个问题：广告效果的归因分析。在查阅资料和寻求解决方案，发现其归因问题可以用强化学习的马尔可夫决策过程来建模。从此才关注和学习这部分。

本章原定题目是：markov，之所以改为强化学习，是因为在广告和推荐策略，尤其是解决广告或商品的冷启动问题时，可以用强化学习的思想来建模。


<br>
### 马尔可夫决策过程
---


<br>
### \\(\epsilon\\)-贪心法
---

\\(\epsilon\\)-贪心法是通过概率对探索和利用进行折中:

|每次尝试时，以\\(\epsilon\\)的概率进行探索，即以均匀挂了随机选取一个摇臂；以\\(1-\epsilon\\)的概率进行利用，即选择平均奖赏最多的摇臂（只选一个）。|
| --- |

用\\(Q(k)\\)纪录摇臂\\(k\\)的平均奖赏。若摇臂\\(k\\)被尝试了\\(n\\)次，得到的奖赏分别为\\(v_1, v_2, \cdots, v_n\\)，那么平均奖赏为：

$$
Q(k) = \frac{1}{n} \sum_{i=1}^{n} v_i
$$

若直接根据上式计算平均奖赏，则需要记录\\(n\\)个奖赏值，\\(n\\)个值都需要保存，空间上不够高效。下面给出一个更高效的方法：对均值进行增量式的计算，即每尝试一次就立即更新\\(Q(k)\\)。

用下标表示尝试的次数，初始时\\(Q_0(k)＝0\\)。对于任意的\\(n \ge 1\\)，若第\\(n-1\\)次尝试后的平均奖赏为\\(Q_{n-1}(k)\\)，则在经过第\\(n\\)次尝试获得\\(v_n\\)后，平均奖赏可更新为：

$$
\begin{align}
Q_n(k) &= \frac{1}{n-1} \left[ (n-1) \cdot Q_{n-1}(k) + v_n \right] \\
&= Q_{n-1}(k) + \frac{1}{n} (v_n - Q_{n-1}(k))
\end{align}
$$

这样，无论摇臂被尝试多少次，只需要记录两个值：已尝试次数\\(n-1\\)和最近平均奖赏\\(Q_{n-1}(k)\\)。

...

---

+ 更多信息请关注：[计算广告与机器学习－CAML 技术共享平台](http://www.52caml.com/)